model:
  encoder:
    - type: !BlockType DOWN_CONV_RELU_MAXPOOL
      params:
        in_channels : 1
        out_channels : 8
        conv_kernel_size : 3
        conv_padding : 1
        pool_kernel_size : 2
        pool_stride : 2

    - type: !BlockType DOWN_CONV_RELU_MAXPOOL
      params:
        in_channels : 8
        out_channels : 32
        conv_kernel_size : 3
        conv_padding : 1
        pool_kernel_size : 2
        pool_stride : 2
    
    - type: !BlockType DOWN_CONV_RELU_MAXPOOL
      params:
        in_channels : 32
        out_channels : 64
        conv_kernel_size : 3
        conv_padding : 1
        pool_kernel_size : 2
        pool_stride : 2

    - type: !BlockType DOWN_CONV_RELU_MAXPOOL
      params:
        in_channels : 64
        out_channels : 128
        conv_kernel_size : 3
        conv_padding : 1
        pool_kernel_size : 2
        pool_stride : 2
        
  decoder:
    - type: !BlockType UP_CONV_RELU_MAXPOOL
      params:
        in_channels : 128
        out_channels : 64
        conv_kernel_size : 3
        conv_padding : 1
        scale_factor : 2
      
    - type: !BlockType UP_CONV_RELU_MAXPOOL
      params:
        in_channels : 64
        out_channels : 32
        conv_kernel_size : 3
        conv_padding : 1
        scale_factor : 2

    - type: !BlockType UP_CONV_RELU_MAXPOOL
      params:
        in_channels : 32
        out_channels : 8
        conv_kernel_size : 3
        conv_padding : 1
        scale_factor : 2

    - type: !BlockType UP_CONV_RELU_MAXPOOL
      params:
        in_channels : 8
        out_channels : 1
        conv_kernel_size : 3
        conv_padding : 1
        scale_factor : 2

data:
  name: MNIST
  img_size: 32
  scale_factor: 2

optimizer:
  name: Adam
  params:
    lr: 0.001
    betas: [0.9, 0.999] 
    weight_decay: 0.0001

scheduler:
  name: CosineAnnealingLR
  params:
    T_max: 30
    eta_min: 0.000001
  is_batch_scheduler: False

loss :
  train_loss_name: SR_IGN_loss_for_train
  test_loss_name: SR_IGN_loss_for_test
  params:
    lam_rec: 15
    lam_idem: 10
    lam_tight: 5
    lam_SR: 15
    a: 3

training:
  batch_size: 64
  epochs: 10